{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 7. Parametric vs. Nonparametric modeling: The $k$-Nearest Neighbors Algorithm\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/blob/main/Lecture_7/Lecture_7_1.ipynb)\n",
    "\n",
    "\n",
    "What is a parameter in a machine learning model?\n",
    "A **model parameter** is a configuration variable that is internal to the model and whose value can be estimated from the given data.\n",
    "* They are required by the model when making predictions.\n",
    "* Their values define the skill of the model on your problem.\n",
    "* They are estimated or learned from historical training data.\n",
    "* They are often not set manually by the practitioner.\n",
    "* They are often saved as part of the learned model.\n",
    "\n",
    "The examples of model parameters include:\n",
    "* The weights in an artificial neural network.\n",
    "* The coefficients in linear regression or logistic regression.\n",
    "\n",
    "Machine learning algorithms are classified into two distinct groups: **parametric** and **nonparametric** models.\n",
    "\n",
    "1. Parametric models deal with discrete values, and nonparametric models use continuous values.\n",
    "2. Parametric models are able to infer the traditional measurements associated with normal distributions including mean, median, and mode. While some nonparametric distributions are normally oriented, often one cannot assume the data comes from a normal distribution.\n",
    "3. Feature engineering is important in parametric models. Because you can poison parametric models if you feed a lot of unrelated features. Nonparametric models handle feature engineering mostly. We can feed all the data we have to those non-parametric algorithms and the algorithm can ignore unimportant features. It would not cause overfitting.\n",
    "4. A parametric model can predict future values using only the parameters. While nonparametric machine learning algorithms are often slower and require large amounts of data, they are rather flexible as they minimize the assumptions they make about the data.\n",
    "\n",
    "The single neuron model, i.e., the Perceptron, linear regression, and logistic regression, and deep neural networks, are all examples of parametric machine learning algorithms. This notebook begins are study of **nonparametric machine learning algorithms**. Some examples of popular nonparametric machine learning algorithms are:\n",
    "* k-Nearest Neighbors\n",
    "* Decision Trees like CART and C4.5\n",
    "* Support Vector Machines\n",
    "\n",
    "This notebook implements the simpliest of these algorithms, namely, the $k$-nearest neighbors algorithm. \n",
    "\n",
    "## $k$-Nearest Neighbors\n",
    "The $k$-nearest neighbors algorithm, or **KNN** for short, is a nonparametric algorithm that assumes that similar data exist in close proximity. In other words, similar things are near to each other. \n",
    "\n",
    "> \"Birds of a feather flock together\"\n",
    "\n",
    "To illustrate this point, let us import the following packages, load the iris dataset, and plot the sepal length versus sepal width. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set theme for plotting\n",
    "sns.set_theme()\n",
    "\n",
    "# Import the data\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Plot the data\n",
    "flowers = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "colors = [\"red\", \"magenta\", \"lightseagreen\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "for species, color in zip(flowers, colors):\n",
    "    temp_df = iris[iris.species == species]\n",
    "    ax.scatter(temp_df.sepal_length,\n",
    "               temp_df.sepal_width,\n",
    "               c = color,\n",
    "               label = species, \n",
    "               )\n",
    "    \n",
    "ax.set_xlabel(\"sepal length [cm]\", fontsize = 15)\n",
    "ax.set_ylabel(\"sepal width [cm]\", fontsize = 15)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Notice in the image above that most of the time, similar data points are close to each other. The KNN algorithm hinges on this assumption being true enough for the algorithm to be useful. KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood— calculating the distance between points on a graph.\n",
    "\n",
    "### The KNN Algorithm\n",
    "1. Load the data\n",
    "2. Initialize K to your chosen number of neighbors\n",
    "3. For each example in the data\n",
    " - 3.1 Calculate the distance between the query example and the current example from the data.\n",
    " - 3.2 Add the distance and the index of the example to an ordered collection\n",
    "4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n",
    "5. Pick the first K entries from the sorted collection\n",
    "6. Get the labels of the selected K entries\n",
    "7. If regression, return the mean of the K labels\n",
    "8. If classification, return the mode of the K labels\n",
    "\n",
    "\n",
    "Before implementing this algorithm we creat a training set and testing set by running the following code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris[[\"petal_length\",\n",
    "          \"petal_width\"]].to_numpy()\n",
    "\n",
    "y = iris[\"species\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For our distance measure, we will choose the **Euclidean distance** defined by the following equation:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{(p - q)^{T} (p - q)}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p, q):\n",
    "    return np.sqrt((p - q) @ (p - q))\n",
    "\n",
    "print(f\"The distance between point 10 and 67 is {distance(X[10], X[67])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "With our distance function, we are now ready to write a function to compute the $k$-nearest neighbors to a given point. This is done in the code cell below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(point, \n",
    "                        training_features, \n",
    "                        training_labels, \n",
    "                        k):\n",
    "    # Create an empty list to store neighbors and distances\n",
    "    neighbors = []\n",
    "    \n",
    "    \n",
    "    for i, p in enumerate(training_features):\n",
    "        d = distance(point, p)\n",
    "        temp_data = [p, training_labels[i], d]\n",
    "        neighbors.append(temp_data)\n",
    "        \n",
    "    neighbors.sort(key = lambda x : x[-1])\n",
    "    \n",
    "    return neighbors[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest_neighbors(X_test[-1], X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we have a function to compute the $k$-nearest neighbors to a point, we can now write a function to predict a target label. However, we should note that KNN can be used for both classification and regression machine learning. This is shown in the code cell below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_Predict(point, \n",
    "                training_features, \n",
    "                training_labels, \n",
    "                k, \n",
    "                regression = False):\n",
    "    \n",
    "    neighbors = k_nearest_neighbors(point, \n",
    "                                    training_features, \n",
    "                                    training_labels, \n",
    "                                    k)\n",
    "    \n",
    "    if regression == False:\n",
    "        labels = [x[1] for x in neighbors]\n",
    "        return max(labels, key = labels.count)\n",
    "    \n",
    "    else:\n",
    "        return sum(x[1] for x in neighbors)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_Predict(X_test[-1], X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we write a function to compute the classification error\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(test_features, \n",
    "                         test_labels,\n",
    "                         training_features, \n",
    "                         training_labels,\n",
    "                         k):\n",
    "    error = 0\n",
    "    for i, point in enumerate(test_features):\n",
    "        error += test_labels[i] != KNN_Predict(point, \n",
    "                                               training_features, \n",
    "                                               training_labels, \n",
    "                                               k)\n",
    "    return error/len(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error(X_test, y_test, X_train, y_train, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right value for K\n",
    "To select the K that’s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before.\n",
    "\n",
    "Here are some things to keep in mind:\n",
    "\n",
    "1. As we decrease the value of K to 1, our predictions become less stable. Just think for a minute, imagine K=1 and we have a query point surrounded by several reds and one green (I’m thinking about the top left corner of the colored plot above), but the green is the single nearest neighbor. Reasonably, we would think the query point is most likely red, but because K=1, KNN incorrectly predicts that the query point is green.\n",
    "2. Inversely, as we increase the value of K, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n",
    "3. In cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among labels, we usually make K an odd number to have a tiebreaker.\n",
    "\n",
    "### Then how to select the optimal K value?\n",
    "* There are no pre-defined statistical methods to find the most favorable value of K.\n",
    "* Initialize a random K value and start computing.\n",
    "* Choosing a small value of K leads to unstable decision boundaries.\n",
    "* The substantial K value is better for classification as it leads to smoothening the decision boundaries.\n",
    "* Derive a plot between error rate and K denoting values in a defined range. Choose the K value having the minimum error rate. \n",
    "\n",
    "### Advantages\n",
    "* The algorithm is simple and easy to implement.\n",
    "* There’s no need to build a model, tune several parameters, or make additional assumptions.\n",
    "* The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section).\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "* The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_k = [x for x in range(3, 26, 2)]\n",
    "errors = [classification_error(X_test,\n",
    "                               y_test,\n",
    "                               X_train,\n",
    "                               y_train, k) for k in possible_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(possible_k, errors, color = 'red')\n",
    "plt.scatter(possible_k, errors, color = 'red')\n",
    "plt.xlabel('k', fontsize = 14)\n",
    "plt.ylabel('Classification Error', fontsize = 14)\n",
    "plt.xticks(possible_k)\n",
    "plt.title(\"The Elbow Method\", fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recommender Systems with $k$-Nearest Neighbors\n",
    "\n",
    "Most internet products we use today are powered by recommender systems. Youtube, Netflix, Amazon, Pinterest, and long list of other internet products all rely on recommender systems to filter millions of contents and make personalized recommendations to their users. Recommender systems are well-studied and proven to provide tremendous values to internet businesses and their consumers. In fact, I was shock at the news that Netflix awarded a $1 million prize to a developer team in 2009, for an algorithm that increased the accuracy of the company’s recommendation system by 10%.\n",
    "\n",
    "Although recommender systems are the secret source for those multi-billion businesses, prototyping a recommender system can be very low cost and doesn’t require a team of scientists. For our very simple example, we will need to load the ```movies_recommendation_data.csv``` stored in this repository. Run the following code and view the movies ```DataFrame```.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('https://raw.githubusercontent.com/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/main/Lecture_7/movies_recommendation_data.csv')\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets now set the ```index``` attribute of our ```DataFrame``` to be the same as the ```MovieName``` column. This can be done by running the following code. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.set_index(\"MovieName\", drop = False, inplace = True)\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can now access the feature vectors for each movie by name! This can be done by using the ```DataFrame``` ```loc``` method as shown with the following code. Note, this returns a ```pandas.Series``` object. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.loc[\"The Terminator\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We need to know all of the column names of our ```DataFrame```. We can find these names by running the following code. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we can right a function to recommend similar movies to a given movie by simply finding the $k$-nearest neighbors to the given feature vector using the function we have already written above! \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recommendations(movie_name, df, k):\n",
    "    categories = ['IMDBRating', \n",
    "                    'Biography', \n",
    "                    'Drama', \n",
    "                    'Thriller',\n",
    "                    'Comedy', \n",
    "                    'Crime', \n",
    "                    'Mystery',\n",
    "                    'History']\n",
    "\n",
    "    X = df[categories].to_numpy()\n",
    "    y = df[\"MovieName\"].to_numpy()\n",
    "\n",
    "    point = df[categories].loc[movie_name].to_numpy()\n",
    "    neighbors = k_nearest_neighbors(point, X, y, k+1)\n",
    "    return [x[1] for x in neighbors[1:]]\n",
    "\n",
    "recommendations = movie_recommendations(\"The Terminator\", movies, 5)\n",
    "\n",
    "print(\"Movie Recommendations\")\n",
    "for i, movie in enumerate(recommendations):\n",
    "    print(f\"{i+1}. {movie}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
